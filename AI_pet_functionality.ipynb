{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from PIL import Image, ImageDraw\n",
    "import torch\n",
    "import numpy as np\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture and Save Owner Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "def capture_and_save_owner_embedding():\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    mtcnn = MTCNN(keep_all=True, device=device)\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        \n",
    "        # Convert to RGB and detect faces\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(frame_rgb)\n",
    "        boxes, _ = mtcnn.detect(pil_img)\n",
    "        \n",
    "        if boxes is not None:\n",
    "            # Extract the face with the largest area (most likely the main subject)\n",
    "            areas = [(box[2] - box[0]) * (box[3] - box[1]) for box in boxes]\n",
    "            largest_face_index = areas.index(max(areas))\n",
    "            largest_face = mtcnn.extract(pil_img, [boxes[largest_face_index]], None)\n",
    "            \n",
    "            # Save the embedding of the largest detected face\n",
    "            owner_embedding = resnet(largest_face).detach()\n",
    "            torch.save(owner_embedding, 'owner_embedding.pt')\n",
    "            \n",
    "            print(\"Owner's face embedding captured and saved.\")\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return owner_embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recognize and Highlight Owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_and_highlight():\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    mtcnn = MTCNN(keep_all=True, device=device)\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "    owner_embedding = torch.load('owner_embedding.pt')\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(frame_rgb)\n",
    "\n",
    "            # Detect faces\n",
    "            boxes, _ = mtcnn.detect(pil_img)\n",
    "            draw = ImageDraw.Draw(pil_img)\n",
    "            if boxes is not None:\n",
    "                faces = mtcnn.extract(pil_img, boxes, None)\n",
    "                embeddings = resnet(faces).detach()\n",
    "\n",
    "                for i, box in enumerate(boxes):\n",
    "                    # Calculate distance to the owner's embedding\n",
    "                    distance = (embeddings[i] - owner_embedding).norm().item()\n",
    "                    if distance < 0.6:  # threshold for recognition, tune based on your dataset\n",
    "                        outline_color = (0, 255, 0)  # Green for owner\n",
    "                    else:\n",
    "                        outline_color = (255, 0, 0)  # Red for others\n",
    "\n",
    "                    draw.rectangle(box.tolist(), outline=outline_color, width=6)\n",
    "\n",
    "            # Display the image\n",
    "            cv_frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "            cv2.imshow('Webcam', cv_frame)\n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is Owner Present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_owner_present():\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    mtcnn = MTCNN(keep_all=True, device=device)\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "    owner_embedding = torch.load('owner_embedding.pt')\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    owner_detected = False  # Flag to indicate if the owner is detected\n",
    "\n",
    "    try:\n",
    "        # Capture a single frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(frame_rgb)\n",
    "\n",
    "            # Detect faces\n",
    "            boxes, _ = mtcnn.detect(pil_img)\n",
    "            if boxes is not None:\n",
    "                faces = mtcnn.extract(pil_img, boxes, None)\n",
    "                embeddings = resnet(faces).detach()\n",
    "\n",
    "                for embedding in embeddings:\n",
    "                    # Calculate distance to the owner's embedding\n",
    "                    distance = (embedding - owner_embedding).norm().item()\n",
    "                    if distance < 0.6:  # threshold for recognition, tune based on your dataset\n",
    "                        owner_detected = True\n",
    "                        break\n",
    "\n",
    "        # Optionally, display the frame with or without marking\n",
    "        # Comment out if not needed\n",
    "        cv_frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow('Webcam', cv_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            pass\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return owner_detected\n",
    "\n",
    "# Example of using the function\n",
    "owner_present = is_owner_present()\n",
    "print(\"Owner present:\", owner_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def is_owner_present():\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    mtcnn = MTCNN(keep_all=True, device=device)\n",
    "    resnet = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "    owner_embedding = torch.load('owner_embedding.pt')\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    owner_detected = False  # Flag to indicate if the owner is detected\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        while time.time() - start_time < 5:  # Check for 5 seconds\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue  # Skip this loop if frame is not captured correctly\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(frame_rgb)\n",
    "\n",
    "            # Detect faces\n",
    "            boxes, _ = mtcnn.detect(pil_img)\n",
    "            if boxes is not None:\n",
    "                faces = mtcnn.extract(pil_img, boxes, None)\n",
    "                embeddings = resnet(faces).detach()\n",
    "\n",
    "                for embedding in embeddings:\n",
    "                    # Calculate distance to the owner's embedding\n",
    "                    distance = (embedding - owner_embedding).norm().item()\n",
    "                    if distance < 0.6:  # threshold for recognition, tune based on your dataset\n",
    "                        owner_detected = True\n",
    "                        break\n",
    "\n",
    "            if owner_detected:\n",
    "                break  # Stop checking if owner is already detected\n",
    "\n",
    "            # Display the frame (optional)\n",
    "            cv_frame = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "            cv2.imshow('Webcam', cv_frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return owner_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Owner present: True\n"
     ]
    }
   ],
   "source": [
    "owner_present = is_owner_present()\n",
    "print(\"Owner present:\", owner_present)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face cascade classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start capturing video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert frame to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale frame to RGB format\n",
    "    rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face ROI (Region of Interest)\n",
    "        face_roi = rgb_frame[y:y + h, x:x + w]\n",
    "\n",
    "        \n",
    "        # Perform emotion analysis on the face ROI\n",
    "        result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "        # Determine the dominant emotion\n",
    "        emotion = result[0]['dominant_emotion']\n",
    "\n",
    "        # Draw rectangle around face and label with predicted emotion\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Real-time Emotion Detection', frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: happy\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: neutral\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: angry\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n",
      "Detected Emotion: sad\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m         cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detected_emotion \u001b[38;5;129;01min\u001b[39;00m continuous_emotion_detection():\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected Emotion:\u001b[39m\u001b[38;5;124m\"\u001b[39m, detected_emotion)\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Additional logic can be added here based on the application's needs\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 25\u001b[0m, in \u001b[0;36mcontinuous_emotion_detection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m gray_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Detect faces in the frame\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mface_cascade\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaleFactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminNeighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (x, y, w, h) \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Extract the face ROI (Region of Interest)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     face_roi \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame[y:y \u001b[38;5;241m+\u001b[39m h, x:x \u001b[38;5;241m+\u001b[39m w], cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)  \u001b[38;5;66;03m# Use original frame to get RGB ROI\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "def continuous_emotion_detection():\n",
    "    # Load face cascade classifier\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Start capturing video\n",
    "    cap = cv2.VideoCapture(0)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue  # If no frame is captured, skip to the next iteration\n",
    "\n",
    "            # Convert frame to grayscale for face detection\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Detect faces in the frame\n",
    "            faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                # Extract the face ROI (Region of Interest)\n",
    "                face_roi = cv2.cvtColor(frame[y:y + h, x:x + w], cv2.COLOR_BGR2RGB)  # Use original frame to get RGB ROI\n",
    "\n",
    "                # Perform emotion analysis on the face ROI\n",
    "                try:\n",
    "                    # Perform emotion analysis on the face ROI\n",
    "                    result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "\n",
    "                    # Determine the dominant emotion\n",
    "                    emotion = result[0]['dominant_emotion']\n",
    "                    yield emotion  # Yield the dominant emotion detected\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in emotion analysis: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Release the capture and close all windows\n",
    "        cap.release()\n",
    "\n",
    "# Example usage\n",
    "for detected_emotion in continuous_emotion_detection():\n",
    "    print(\"Detected Emotion:\", detected_emotion)\n",
    "    # Additional logic can be added here based on the application's needs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBot: \u001b[39m\u001b[38;5;124m\"\u001b[39m, bot_output)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m, in \u001b[0;36mchat\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m():\n\u001b[0;32m      5\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/DialoGPT-medium\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmicrosoft/DialoGPT-medium\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Chatbot initialization message\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWoof! I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm your chat pet. Type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to stop playing with me.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\transformers\\modeling_utils.py:3335\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3332\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3333\u001b[0m         \u001b[38;5;66;03m# This repo has no safetensors file of any kind, we switch to PyTorch.\u001b[39;00m\n\u001b[0;32m   3334\u001b[0m         filename \u001b[38;5;241m=\u001b[39m _add_variant(WEIGHTS_NAME, variant)\n\u001b[1;32m-> 3335\u001b[0m         resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3336\u001b[0m             pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs\n\u001b[0;32m   3337\u001b[0m         )\n\u001b[0;32m   3338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(WEIGHTS_NAME, variant):\n\u001b[0;32m   3339\u001b[0m     \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n\u001b[0;32m   3340\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[0;32m   3341\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m   3342\u001b[0m         _add_variant(WEIGHTS_INDEX_NAME, variant),\n\u001b[0;32m   3343\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs,\n\u001b[0;32m   3344\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\huggingface_hub\\file_download.py:1492\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1490\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1492\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\huggingface_hub\\file_download.py:535\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    533\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    536\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    537\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\urllib3\\response.py:934\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 934\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m    937\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\urllib3\\response.py:877\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    875\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 877\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    879\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\urllib3\\response.py:812\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    809\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 812\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\site-packages\\urllib3\\response.py:797\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buffer\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    795\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\MustiTanvir\\.conda\\envs\\deepface-env\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def chat():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "    # Chatbot initialization message\n",
    "    print(\"Woof! I'm your chat pet. Type 'quit' to stop playing with me.\")\n",
    "    \n",
    "    chat_history_ids = None\n",
    "\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Woof woof! Bye!\")\n",
    "            break\n",
    "\n",
    "        # Custom responses for pet-like behavior\n",
    "        if \"what are you\" in user_input.lower():\n",
    "            print(\"Bot: I'm your friendly chat pet! I like pats and treats!\")\n",
    "            continue\n",
    "        elif \"do you like\" in user_input.lower():\n",
    "            print(\"Bot: I love everything about you!\")\n",
    "            continue\n",
    "\n",
    "        # Encode and generate response\n",
    "        new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=1) if chat_history_ids is not None else new_user_input_ids\n",
    "        chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "        bot_output = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "        print(\"Bot: \", bot_output)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "\n",
    "def emotion_detection():\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Cannot open webcam\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
    "            faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                face_roi = rgb_frame[y:y+h, x:x+w]\n",
    "                try:\n",
    "                    result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "                    emotion = result['dominant_emotion']\n",
    "                    yield emotion\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in emotion detection: {e}\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n",
      "Error in emotion detection: list indices must be integers or slices, not str\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m emotions \u001b[38;5;241m=\u001b[39m emotion_detection()\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m detected_emotion \u001b[38;5;129;01min\u001b[39;00m emotions:\n\u001b[0;32m      4\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI sense you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetected_emotion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. What happened?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m detected_emotion \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msad\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid I do something wrong?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m detected_emotion \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangry\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou seem happy! Want to play?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m detected_emotion \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhappy\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \\\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow are you feeling?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPet senses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m, in \u001b[0;36memotion_detection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m         ret, frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m     15\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emotions = emotion_detection()\n",
    "\n",
    "for detected_emotion in emotions:\n",
    "    user_input = f\"I sense you're {detected_emotion}. What happened?\" if detected_emotion == 'sad' else \\\n",
    "        \"Did I do something wrong?\" if detected_emotion == 'angry' else \\\n",
    "        \"You seem happy! Want to play?\" if detected_emotion == 'happy' else \\\n",
    "        \"How are you feeling?\"\n",
    "\n",
    "    print(f\"Pet senses: {user_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def chat():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\", padding_side='left')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "    print(\"Woof! I'm your chat pet. Type 'quit' to stop playing with me.\")\n",
    "    chat_history_ids = None\n",
    "\n",
    "    # Start the emotion detection generator\n",
    "    emotions = emotion_detection()\n",
    "\n",
    "    try:\n",
    "        for detected_emotion in emotions:\n",
    "            user_input = f\"I sense you're {detected_emotion}. What happened?\" if detected_emotion == 'sad' else \\\n",
    "                         \"Did I do something wrong?\" if detected_emotion == 'angry' else \\\n",
    "                         \"You seem happy! Want to play?\" if detected_emotion == 'happy' else \\\n",
    "                         \"How are you feeling?\"\n",
    "\n",
    "            print(f\"Pet senses: {user_input}\")\n",
    "\n",
    "            new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "            bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=1) if chat_history_ids is not None else new_user_input_ids\n",
    "            chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "            bot_output = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "            print(\"Bot: \", bot_output)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Chat interrupted.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
